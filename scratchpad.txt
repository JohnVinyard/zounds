Installation
====================================
- In the documentation, update path to audio files in quickstart search example
- rst rendering on BitBucket homepage is broken


Musical Pattern game
====================================

Tile Map Problems
---------------------------------
- fix bark contrast problem
	- ideas:
		- Record the loudest "pixel" in each pattern, and normalize according to that
		- Use a log scale to "compand" pixels.  This is the cheapest, but means
		  that contrast is reduced everywhere
		- deliver information about the loudest "pixel" in each block, and
		  either:
		  	a) alter the image data client side
		  	b) apply a filter to the images.

- audio control
	- move around in a file
	- create leaf patterns by selecting segments and saving them
	
- server-side rendering
	- Why does playraw block for much longer than the expected length of a rendered
	   patterns?
	- can I speed up scheduling?
		- cache values and interpolations arrays
		- cache times as a numpy array. Zound.interpret_times should work for numpy arrays too
	- Change event2 to event
	- Convolution
	- BiQuad Filter
	- Compressor

- pattern analysis
	- There should be a single analysis worker process running, and it should use
	  the multiprocess TemplateMatch algorithm

- WebAudio pattern rendering
- server-side -> browser listening tests / comparison
- Web Audio delay unit and blog post
- Facebook and Google login
- Asynchronous upload
- Update indexes on-the-fly


- YIN implementation
- Chroma implementation
- Stupid onset detection
- Stupid BPM estimation
- How do I index things like pitch, bpm, chroma, etc?


- JACK client should do resampling, if necessary

- Coffeescript implementation of MusicPattern code
- How do I queue and analyze incoming patterns?

- Pattern construction is cumbersome
- Move tests that have nothing to do with data storage/access into the model package
- Address.contiguous() method which helps to answer the question: 
  "Should this pattern be re-analyzed and stored in the frames database?"


TemplateMatch tweaks
====================================
- Similarity search should be a web service

- Upload/record query sounds in websearch.py

- dragger/slider for playhead positioning

- Display timecodes in websearch interface

- How does correlation compare with euclidean distance once all patches have
  unit norm, both in terms of speed and accuracy?
  
- Once the Templates are learned, take the DCT of each and sparsify to speed
  up analysis?
  
- How about the distance between random samples of the patches?

- 2d fft performance (as compared to TemplateMatch)?
- Try RBM and autoencoder for learning patch centroids, instead of KMeans
- Multiscale kmeans, i.e., do TemplateMatch on multiple scales
- Expectation-Maximization clustering
- Try lowpass filter subtraction method for spectrogram sharpening

- There's something wrong about the way I'm doing searches with the TemplateMatch
  semantic hashing features. It's most obvious when silence (or near-silence) is
  present in a query.  These searches demonstrate problems:
  
    http://zounds.co/?q=107750bfe7774f9aa1f8b778944b4ca7:130:260
    
	http://localhost:8080/zoundsapp?q=bcdce3a074c24708bada301df9b764a2:0:124
	http://localhost:8080/zoundsapp?q=62c67ee8b3ad4d648e2b411a77f4e661:0:114
	http://localhost:8080/zoundsapp?q=e5acc136103245b2982506a2b37955bd:169:290
	http://localhost:8080/zoundsapp?q=f62dc6f1bdb04f5bad3b0984a1b3c0b9:0:95
	http://localhost:8080/zoundsapp?q=745f909faccd4cee98cab19139b5bb66:14:103
	http://localhost:8080/zoundsapp?q=1b902c831b564dbfb54055f7f29d1ea4:67:115
	http://localhost:8080/zoundsapp?q=b376c4575f6e41538e5b31d876215d6d:0:86 (need better resolution)
	
Audio Encoding
===================================
Why is FFT resolution insufficient at low frequencies?
Why does VQ coding work so poorly?
How can upsampling be performed without aliasing?
How does the human brain represent noise?


Good Searches
=================
http://localhost/?q=8f906adb408f4475b68fee94447876af:31:178
http://localhost/?q=087744117a6b40b2ac40a17802b2968c:14:65


- Add option to search.py to draw sounds randomly from a database, or from
  a directory
  	
- Limit search sizes

- Limit size of image and audio requests

- start a planning document for the higher level/ more abstract stuff


Storage of variable length data
====================================
Encoder <-> Decoder pairs

BOTH - Ogg Vorbis or MP3.  Audio is encoded, and can be decoded back to audio
ONLY ENCODE - Loudness.  *Way* too much information is discarded to synthesize 
							the signal.
ONLY DECODE - Parameters to a synthesizer.  These can only be decoded back into
				audio, but not the other way.

Ways of representing time:
------------------------------------
Constant or variable time X Constant or variable data length

C = contiguous memory
T = Time-Binary tree.

ct  | C | T |
   ----------
vt  | T | T |
   ----------
     cd   vd


In the left-hand column, sampling rate is defined as <= the "root" input's
sample rate, e.g., audio at 44.1khz

For ct & vd and vt & vd, the b-tree consists of byte offsets for time ranges, the
leaves being byte offsets for individual data chunks




 